
Any-2-any [[LLM]]s are trying to make LLMs accept multiple inputs and outputs. There are currently some open-source models that tackle this problem. The following is a quick rundown of open source models and their respective architectures.

| Name             | Inputs                                             | Outputs                             | Open Source | Architecture                                                                                                             | Architecture Diagram       |
| ---------------- | -------------------------------------------------- | ----------------------------------- | ----------- | ------------------------------------------------------------------------------------------------------------------------ | -------------------------- |
| [[LlaVa]]        | text + image                                       | text                                | Yes         | Vision Encoder ([[CLIP]] ViT-L/14) + [[Vicuna]]                                                                          | ![[llava diagram.png]]     |
| [[PandaGPT]]     | text + image/video + audio + depth + thermal + IMU | text                                | Yes         | Multimodal encoders from ImageBind + [[Vicuna]]                                                                          | ![[pandagpt diagram.png]]  |
| [[Mini-GPT4]]    | text + image                                       | text                                | Yes         | Q-Former & ViT + [[Vicuna]]                                                                                              | ![[minigpt-4 diagram.png]] |
| [[NExT-GPT]]     | text + image + audio + video                       | text + image + audio + video        | Yes         | Multimodal encoders from [[ImageBind]] + [[Vicuna]]                                                                      | ![[next-gpt diagram.png]]  |
| [[AnyGPT]]       | text + image + audio                               | text + image + audio (speech/music) | Not yet     | SEED tokenizer + SpeechTokenizer + Encodec + LLaMA-2 7B                                                                  | ![[anygpt diagram.png]]    |
| [[IDEFICS]]      | images + video + text                              | text                                | Yes         | OpenClip + LlaMA                                                                                                         | ![[flamingo diagram.png]]  |
| [[OpenFlamingo]] | images + text                                      | text                                | Yes         | CLIP ViT-L/14 + MPT-1B / RedPajama3B / MPT-7B                                                                            | ![[flamingo diagram.png]]  |
| [[AnyMal]]       | image + audio + video + text + IMU                 | text                                | No          | CLIP ViT-L + ViT-G + DinoV2<br>(image) + CLAP (audio) + IMU2CLIP (IMU motion sensor) +  Intervideo (video) + LLaMA-2 70B | ![[AnyMal diagram.png]]    |


I love that most of the efforts from this era specifically use Vicuna because [[Mistral]] seems to still have not been a thing.