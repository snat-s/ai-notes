> An [[LLM]] "combines a [[Vision Encoder]] and [[Vicuna]] for general-purpose visual and language understanding, achieving impressive chat capabilities mimicking spirits of the multimodal GPT-4 and setting a new state-of-the-art accuracy on Science QA."

The architecture diagram:
![[llava diagram.png]]